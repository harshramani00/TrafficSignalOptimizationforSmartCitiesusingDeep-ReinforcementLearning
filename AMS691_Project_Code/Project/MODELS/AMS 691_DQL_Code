{"cells":[{"cell_type":"markdown","metadata":{"id":"4e7hCvwlC6ud"},"source":["# 1. IMPORT DEPENDENCIES"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8c67L43seqiZ","outputId":"e9e4677f-daec-476c-d620-790fda075ff9","executionInfo":{"status":"ok","timestamp":1733260182203,"user_tz":300,"elapsed":17194,"user":{"displayName":"Aryan Patil","userId":"15902130247588668926"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","sumo is already the newest version (1.12.0+dfsg1-1).\n","sumo-doc is already the newest version (1.12.0+dfsg1-1).\n","sumo-tools is already the newest version (1.12.0+dfsg1-1).\n","0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n","Requirement already satisfied: traci in /usr/local/lib/python3.10/dist-packages (1.21.0)\n","Requirement already satisfied: sumolib>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from traci) (1.21.0)\n"]}],"source":["# Install SUMO and other required packages\n","\n","!apt-get update -qq\n","\n","!apt-get install -y sumo sumo-tools sumo-doc\n","\n","!pip install stable-baselines3 --quiet\n","\n","!pip install traci"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6ixVf6vHva5"},"outputs":[],"source":["import numpy as np\n","\n","import random\n","\n","from collections import deque\n","\n","\n","import os\n","\n","import sys\n","\n","import traci\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v8s1iClVvkA"},"outputs":[],"source":["# Set SUMO_HOME environment variable\n","\n","os.environ['SUMO_HOME'] = '/usr/share/sumo'\n","\n","\n","\n","# Import traci module\n","\n","if 'SUMO_HOME' in os.environ:\n","\n","    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n","\n","    sys.path.append(tools)\n","\n","else:\n","\n","    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")"]},{"cell_type":"markdown","metadata":{"id":"1EMrYt_GDAJX"},"source":["# 2. CREATING ENVIRONMENT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VIBRHPMOb-N","outputId":"e42e6623-f253-4e6b-e655-7e1eb2c44c3d","executionInfo":{"status":"ok","timestamp":1733260184648,"user_tz":300,"elapsed":2449,"user":{"displayName":"Aryan Patil","userId":"15902130247588668926"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["SUMO files created successfully.\n","Running SUMO simulation...\n","SUMO simulation completed.\n"]}],"source":["import os\n","import subprocess\n","import sys\n","import traci\n","\n","def create_sumo_files():\n","    # Create nodes file with traffic lights\n","    with open('my_nodes.nod.xml', 'w') as f:\n","        f.write('''<nodes>\n","    <node id=\"1\" x=\"0\" y=\"0\" type=\"priority\"/>\n","    <node id=\"2\" x=\"100\" y=\"0\" type=\"priority\"/>\n","    <node id=\"3\" x=\"25\" y=\"50\" type=\"priority\"/>\n","    <node id=\"4\" x=\"25\" y=\"-50\" type=\"priority\"/>\n","    <node id=\"5\" x=\"75\" y=\"50\" type=\"priority\"/>\n","    <node id=\"6\" x=\"75\" y=\"-50\" type=\"priority\"/>\n","    <node id=\"7\" x=\"25\" y=\"0\" type=\"traffic_light\"/>\n","    <node id=\"8\" x=\"75\" y=\"0\" type=\"traffic_light\"/>\n","\n","    <node id=\"e1\" x=\"0\" y=\"25\" type=\"priority\"/>\n","    <node id=\"e2\" x=\"25\" y=\"0\" type=\"priority\"/>\n","</nodes>''')\n","\n","    # Create edges file\n","    with open('my_edges.edg.xml', 'w') as f:\n","        f.write('''<edges>\n","    <edge id=\"1to7\" from=\"1\" to=\"7\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"7to8\" from=\"7\" to=\"8\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"8to2\" from=\"8\" to=\"2\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"2to8\" from=\"2\" to=\"8\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"8to7\" from=\"8\" to=\"7\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"7to1\" from=\"7\" to=\"1\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"3to7\" from=\"3\" to=\"7\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"7to4\" from=\"7\" to=\"4\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"4to7\" from=\"4\" to=\"7\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"7to3\" from=\"7\" to=\"3\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"5to8\" from=\"5\" to=\"8\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"8to6\" from=\"8\" to=\"6\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"6to8\" from=\"6\" to=\"8\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","    <edge id=\"8to5\" from=\"8\" to=\"5\" priority=\"1\" numLanes=\"2\" speed=\"13.89\"/>\n","\n","    <edge id=\"e1to7\" from=\"e1\" to=\"7\" priority=\"1\" numLanes=\"1\" speed=\"20.0\"/>\n","    <edge id=\"7toe1\" from=\"7\" to=\"e1\" priority=\"1\" numLanes=\"1\" speed=\"20.0\"/>\n","</edges>''')\n","\n","    # Generate network file using netconvert\n","    subprocess.run(['netconvert', '--node-files=my_nodes.nod.xml', '--edge-files=my_edges.edg.xml', '--output-file=my_network.net.xml', '--tls.guess', '--tls.default-type', 'static'])\n","\n","    # Create routes file with more vehicles\n","    with open('my_routes.rou.xml', 'w') as f:\n","        f.write('''<routes>\n","\n","     <!-- Vehicles -->\n","\n","    <vType id=\"car\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n","    <vType id=\"emergency_car\" accel=\"1.2\" decel=\"6.0\" sigma=\"0.5\" length=\"6\" minGap=\"2.5\" maxSpeed=\"25.0\" guiShape=\"emergency\"/>\n","\n","    <!-- Common Routes -->\n","\n","    <route id=\"route0\" edges=\"1to7 7to8 8to2\"/>\n","    <route id=\"route1\" edges=\"2to8 8to7 7to1\"/>\n","    <route id=\"route2\" edges=\"3to7 7to4\"/>\n","    <route id=\"route3\" edges=\"4to7 7to3\"/>\n","    <route id=\"route4\" edges=\"5to8 8to6\"/>\n","    <route id=\"route5\" edges=\"6to8 8to5\"/>\n","\n","\n","    <!-- Emergency Routes -->\n","\n","    <route id=\"emergency_route1\" edges=\"e1to7 7to8 8to2\"/>\n","    <route id=\"emergency_route2\" edges=\" 2to8 8to7 7toe1\"/>\n","\n","\n","\n","    <!-- Complex Routes -->\n","\n","    <route id=\"complex_route1\" edges=\"1to7 7to4\"/>\n","    <route id=\"complex_route2\" edges=\"1to7 7to3\"/>\n","    <route id=\"complex_route3\" edges=\"1to7 7to8 8to6\"/>\n","    <route id=\"complex_route4\" edges=\"1to7 7to8 8to5\"/>\n","    <route id=\"complex_route5\" edges=\"1to7 7to8 8to2\"/>\n","\n","    <route id=\"complex_route6\" edges=\"2to8 8to6\"/>'\n","    <route id=\"complex_route7\" edges=\"2to8 8to5\"/>\n","    <route id=\"complex_route8\" edges=\"2to8 8to7 7to4\"/>\n","    <route id=\"complex_route9\" edges=\"2to8 8to7 7to3\"/>\n","    <route id=\"complex_route10\" edges=\"2to8 8to7 7to1\"/>\n","\n","    <route id=\"complex_route11\" edges=\"3to7 7to8 8to6\"/>\n","    <route id=\"complex_route12\" edges=\"3to7 7to8 8to5\"/>\n","    <route id=\"complex_route13\" edges=\"3to7 7to8 8to2\"/>\n","    <route id=\"complex_route14\" edges=\"3to7 7to4\"/>\n","\n","    <route id=\"complex_route15\" edges=\"4to7 7to8 8to6\"/>\n","    <route id=\"complex_route16\" edges=\"4to7 7to8 8to5\"/>\n","    <route id=\"complex_route17\" edges=\"4to7 7to8 8to2\"/>\n","    <route id=\"complex_route18\" edges=\"4to7 7to3\"/>\n","\n","    <route id=\"complex_route19\" edges=\"5to8 8to2\"/>\n","    <route id=\"complex_route20\" edges=\"5to8 8to7 7to4\"/>\n","    <route id=\"complex_route21\" edges=\"5to8 8to7 7to3\"/>\n","    <route id=\"complex_route22\" edges=\"5to8 8to6\"/>\n","\n","    <route id=\"complex_route23\" edges=\"6to8 8to2\"/>\n","    <route id=\"complex_route24\" edges=\"6to8 8to7 7to4\"/>\n","    <route id=\"complex_route25\" edges=\"6to8 8to7 7to3\"/>\n","    <route id=\"complex_route26\" edges=\"6to8 8to5\"/>\n","\n","\n","    <!-- Flows -->\n","\n","    <flow id=\"flow0\" type=\"car\" route=\"route0\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"flow1\" type=\"car\" route=\"route1\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"flow2\" type=\"car\" route=\"route2\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"flow3\" type=\"car\" route=\"route3\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"flow4\" type=\"car\" route=\"route4\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"flow5\" type=\"car\" route=\"route5\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","    <flow id=\"emergency_flow1\" type=\"emergency_car\" route=\"emergency_route1\" begin=\"0\" end=\"3600\" vehsPerHour=\"1\"/>\n","    <flow id=\"emergency_flow2\" type=\"emergency_car\" route=\"emergency_route2\" begin=\"0\" end=\"3600\" vehsPerHour=\"2\"/>\n","\n","    <flow id=\"complex_flow1\" type=\"car\" route=\"complex_route1\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow2\" type=\"car\" route=\"complex_route2\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow3\" type=\"car\" route=\"complex_route3\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow4\" type=\"car\" route=\"complex_route4\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow5\" type=\"car\" route=\"complex_route5\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","    <flow id=\"complex_flow6\" type=\"car\" route=\"complex_route6\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow7\" type=\"car\" route=\"complex_route7\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow8\" type=\"car\" route=\"complex_route8\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow9\" type=\"car\" route=\"complex_route9\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow10\" type=\"car\" route=\"complex_route10\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","    <flow id=\"complex_flow11\" type=\"car\" route=\"complex_route11\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow12\" type=\"car\" route=\"complex_route12\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow13\" type=\"car\" route=\"complex_route13\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow14\" type=\"car\" route=\"complex_route14\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","    <flow id=\"complex_flow15\" type=\"car\" route=\"complex_route15\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow16\" type=\"car\" route=\"complex_route16\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow17\" type=\"car\" route=\"complex_route17\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow18\" type=\"car\" route=\"complex_route18\" begin=\"0\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","    <flow id=\"complex_flow19\" type=\"car\" route=\"complex_route19\" begin=\"400\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow20\" type=\"car\" route=\"complex_route20\" begin=\"400\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow21\" type=\"car\" route=\"complex_route21\" begin=\"800\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow22\" type=\"car\" route=\"complex_route22\" begin=\"800\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","    <flow id=\"complex_flow23\" type=\"car\" route=\"complex_route23\" begin=\"1500\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow24\" type=\"car\" route=\"complex_route24\" begin=\"1500\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow25\" type=\"car\" route=\"complex_route25\" begin=\"1500\" end=\"3600\" vehsPerHour=\"50\"/>\n","    <flow id=\"complex_flow26\" type=\"car\" route=\"complex_route26\" begin=\"1500\" end=\"3600\" vehsPerHour=\"50\"/>\n","\n","</routes>''')\n","\n","    # Create SUMO configuration file\n","    with open('my_simulation.sumocfg', 'w') as f:\n","        f.write('''<configuration>\n","    <input>\n","        <net-file value=\"my_network.net.xml\"/>\n","        <route-files value=\"my_routes.rou.xml\"/>\n","    </input>\n","    <time>\n","        <begin value=\"0\"/>\n","        <end value=\"3600\"/>\n","    </time>\n","    <report>\n","        <verbose value=\"true\"/>\n","        <no-step-log value=\"true\"/>\n","    </report>\n","</configuration>''')\n","\n","\n","def run_sumo_simulation():\n","    # Run SUMO simulation\n","    subprocess.run(['sumo', '-c', 'my_simulation.sumocfg'])\n","\n","\n","if __name__ == \"__main__\":\n","    # Check if SUMO_HOME is set\n","    if 'SUMO_HOME' not in os.environ:\n","        print(\"Please set the SUMO_HOME environment variable\")\n","        exit(1)\n","\n","    # Add SUMO tools to Python path\n","    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n","    sys.path.append(tools)\n","\n","    # Create SUMO files\n","    create_sumo_files()\n","    print(\"SUMO files created successfully.\")\n","\n","    # Run SUMO simulation\n","    print(\"Running SUMO simulation...\")\n","    run_sumo_simulation()\n","    print(\"SUMO simulation completed.\")"]},{"cell_type":"markdown","metadata":{"id":"oWKQoYEiDEiR"},"source":["# 3. OBTAINING HISTORY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiPGZaozr-CB","outputId":"40adccba-7cac-4b60-a61d-a36427da265e","executionInfo":{"status":"ok","timestamp":1733260202022,"user_tz":300,"elapsed":17381,"user":{"displayName":"Aryan Patil","userId":"15902130247588668926"}}},"outputs":[{"output_type":"stream","name":"stdout","text":[" Retrying in 1 seconds\n"]}],"source":["import traci\n","\n","def generate_history(steps=3600):\n","    history = []\n","    traci.start(['sumo', '-c', 'my_simulation.sumocfg'])\n","\n","    for step in range(steps):\n","        traci.simulationStep()\n","\n","        state = {\n","            'time_step': step,\n","            'traffic_light_phases': {\n","                '7': traci.trafficlight.getPhase('7'),\n","                '8': traci.trafficlight.getPhase('8')\n","            },\n","            'vehicle_count': traci.vehicle.getIDCount(),\n","            'average_speed': traci.vehicle.getIDCount() > 0 and sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / traci.vehicle.getIDCount() or 0,\n","            'queue_lengths': {\n","                '1to7': traci.edge.getLastStepHaltingNumber('1to7'),\n","                '3to7': traci.edge.getLastStepHaltingNumber('3to7'),\n","                '5to8': traci.edge.getLastStepHaltingNumber('5to8'),\n","                'e1to7': traci.edge.getLastStepHaltingNumber('e1to7'),\n","                '7toe1': traci.edge.getLastStepHaltingNumber('7toe1'),\n","                '2to8': traci.edge.getLastStepHaltingNumber('2to8'),\n","                '8to2': traci.edge.getLastStepHaltingNumber('8to2'),\n","                '8to7': traci.edge.getLastStepHaltingNumber('8to7'),\n","                '7to1': traci.edge.getLastStepHaltingNumber('7to1'),\n","                '7to4': traci.edge.getLastStepHaltingNumber('7to4'),\n","                '4to7': traci.edge.getLastStepHaltingNumber('4to7'),\n","                '7to3': traci.edge.getLastStepHaltingNumber('7to3'),\n","                '8to6': traci.edge.getLastStepHaltingNumber('8to6'),\n","                '6to8': traci.edge.getLastStepHaltingNumber('6to8'),\n","                '8to5': traci.edge.getLastStepHaltingNumber('8to5'),\n","                'e1to7': traci.edge.getLastStepHaltingNumber('e1to7'),\n","                '7toe1': traci.edge.getLastStepHaltingNumber('7toe1')\n","\n","            }\n","\n","            }\n","\n","\n","        history.append(state)\n","\n","    traci.close()\n","    return history\n","\n","def run_sumo_simulation():\n","    history = generate_history()\n","    return history\n","\n","history = run_sumo_simulation()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Wr3DR5OtsDD","outputId":"a3830767-cb14-4ca0-f002-54f289fdae49"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Retrying in 1 seconds\n"]}],"source":["import traci\n","import random\n","import numpy as np\n","\n","def calculate_reward(state):\n","    avg_speed = state['average_speed']\n","    total_queue = sum(state['queue_lengths'].values())\n","    return avg_speed - 0.1 * total_queue\n","\n","def discretize_state(state):\n","    def categorize(value, ranges):\n","        for i, r in enumerate(ranges):\n","            if value <= r:\n","                return i\n","        return len(ranges)\n","\n","    discretized = {\n","        'traffic_light_phases': state['traffic_light_phases'],\n","        'vehicle_count': categorize(state['vehicle_count'], [10, 30, 40]),\n","        'average_speed': categorize(state['average_speed'], [0.5, 1, 2]),\n","        'queue_lengths': {edge: categorize(length, [0, 5, 8]) for edge, length in state['queue_lengths'].items()}\n","    }\n","    return tuple(discretized.values())\n","\n","def generate_history(steps=3600):\n","    history = []\n","    traci.start(['sumo', '-c', 'my_simulation.sumocfg'])\n","\n","    total_waiting_time = 0\n","    total_travel_time = 0\n","    total_distance_travelled = 0\n","    total_fuel_consumption = 0\n","    total_co2_emission = 0\n","\n","    for step in range(steps):\n","        current_state = {\n","            'time_step': step,\n","            'traffic_light_phases': {\n","                '7': traci.trafficlight.getPhase('7'),\n","                '8': traci.trafficlight.getPhase('8')\n","            },\n","            'vehicle_count': traci.vehicle.getIDCount(),\n","            'average_speed': traci.vehicle.getIDCount() > 0 and sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / traci.vehicle.getIDCount() or 0,\n","            'queue_lengths': {\n","                edge: traci.edge.getLastStepHaltingNumber(edge) for edge in\n","                ['1to7', '3to7', '5to8', 'e1to7', '7toe1', '2to8', '8to2', '8to7', '7to1', '7to4', '4to7', '7to3', '8to6', '6to8', '8to5']\n","            }\n","        }\n","\n","        action = {\n","            '7': random.randint(0, 3),\n","            '8': random.randint(0, 3)\n","        }\n","        traci.trafficlight.setPhase('7', action['7'])\n","        traci.trafficlight.setPhase('8', action['8'])\n","\n","        traci.simulationStep()\n","\n","        next_state = {\n","            'time_step': step + 1,\n","            'traffic_light_phases': {\n","                '7': traci.trafficlight.getPhase('7'),\n","                '8': traci.trafficlight.getPhase('8')\n","            },\n","            'vehicle_count': traci.vehicle.getIDCount(),\n","            'average_speed': traci.vehicle.getIDCount() > 0 and sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / traci.vehicle.getIDCount() or 0,\n","            'queue_lengths': {\n","                edge: traci.edge.getLastStepHaltingNumber(edge) for edge in\n","                ['1to7', '3to7', '5to8', 'e1to7', '7toe1', '2to8', '8to2', '8to7', '7to1', '7to4', '4to7', '7to3', '8to6', '6to8', '8to5']\n","            }\n","        }\n","\n","        reward = calculate_reward(next_state)\n","        current_state = discretize_state(current_state)\n","        next_state = discretize_state(next_state)\n","        history.append((current_state, action, reward, next_state))\n","\n","        # Update metrics\n","        total_waiting_time += sum(traci.vehicle.getWaitingTime(v) for v in traci.vehicle.getIDList())\n","        total_travel_time += sum(traci.vehicle.getAccumulatedWaitingTime(v) for v in traci.vehicle.getIDList())\n","        total_distance_travelled += sum(traci.vehicle.getDistance(v) for v in traci.vehicle.getIDList())\n","        total_fuel_consumption += sum(traci.vehicle.getFuelConsumption(v) for v in traci.vehicle.getIDList())\n","        total_co2_emission += sum(traci.vehicle.getCO2Emission(v) for v in traci.vehicle.getIDList())\n","\n","\n","\n","    # Calculate final metrics\n","    metrics = {\n","        'average_waiting_time': total_waiting_time / steps,\n","        'average_travel_time': total_travel_time / steps,\n","        'average_speed': total_distance_travelled / total_travel_time if total_travel_time > 0 else 0,\n","        'total_distance_travelled': total_distance_travelled,\n","        'average_fuel_consumption': total_fuel_consumption / steps,\n","        'average_co2_emission': total_co2_emission / steps,\n","        'throughput': len(set(v for state, _, _, _ in history for v in traci.simulation.getArrivedIDList()))\n","    }\n","\n","    traci.close()\n","    return history, metrics\n","\n","def run_sumo_simulation():\n","    history, metrics = generate_history()\n","    return history, metrics\n","\n","history, metrics = run_sumo_simulation()\n","print(\"Simulation metrics:\", metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN-pNVO2sFvl"},"outputs":[],"source":["history[9]\n"]},{"cell_type":"markdown","metadata":{"id":"l9HDoVykDT_1"},"source":["# 4. Q-Learning Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tg4aAQVLzaEH"},"outputs":[],"source":["import traci\n","import random\n","import numpy as np\n","\n","# Q-learning parameters\n","LEARNING_RATE = 0.1\n","DISCOUNT_FACTOR = 0.9\n","EPSILON = 0.1\n","NUM_EPISODES = 3\n","\n","def discretize_state(state):\n","    def categorize(value, ranges):\n","        for i, r in enumerate(ranges):\n","            if value <= r:\n","                return i\n","        return len(ranges)\n","\n","    discretized = (\n","        state['traffic_light_phases']['7'],\n","        state['traffic_light_phases']['8'],\n","        categorize(state['vehicle_count'], [10, 30, 40]),\n","        categorize(state['average_speed'], [0.5, 1, 2]),\n","        tuple(categorize(length, [0, 5, 8]) for length in state['queue_lengths'].values())\n","    )\n","    return discretized\n","\n","def calculate_reward(state):\n","    avg_speed = state['average_speed']\n","    total_queue = sum(state['queue_lengths'].values())\n","    waiting_vehicles = sum(traci.edge.getLastStepHaltingNumber(edge) for edge in state['queue_lengths'])\n","    return avg_speed - 0.01 * total_queue - 0.2 * waiting_vehicles\n","\n","def q_learning(num_episodes=NUM_EPISODES, steps_per_episode=3600):\n","    Q = {}\n","    metrics_history = []\n","\n","    for episode in range(num_episodes):\n","        try:\n","            traci.start(['sumo', '-c', 'my_simulation.sumocfg'])\n","            state = get_state()\n","            state = discretize_state(state)\n","\n","            total_reward = 0\n","\n","            for step in range(steps_per_episode):\n","                if random.random() < EPSILON:\n","                    action = random.choice([0, 1, 2, 3])\n","                else:\n","                    action = np.argmax(Q.get(state, np.zeros(4)))\n","\n","                take_action(action)\n","                traci.simulationStep()\n","\n","                next_state = get_state()\n","                reward = calculate_reward(next_state)\n","                next_state = discretize_state(next_state)\n","\n","                if state not in Q:\n","                    Q[state] = np.zeros(4)\n","\n","                Q[state][action] = Q[state][action] + LEARNING_RATE * (reward + DISCOUNT_FACTOR * np.max(Q.get(next_state, np.zeros(4))) - Q[state][action])\n","\n","                state = next_state\n","                total_reward += reward\n","\n","            metrics = calculate_metrics()\n","            metrics['episode'] = episode\n","            metrics['total_reward'] = total_reward\n","            metrics_history.append(metrics)\n","\n","            print(f\"Episode {episode} completed. Total reward: {total_reward}\")\n","\n","        except traci.exceptions.FatalTraCIError as e:\n","            print(f\"TraCI error in episode {episode}: {e}\")\n","        finally:\n","            traci.close()\n","\n","    return Q, metrics_history\n","\n","def get_state():\n","    return {\n","        'traffic_light_phases': {\n","            '7': traci.trafficlight.getPhase('7'),\n","            '8': traci.trafficlight.getPhase('8')\n","        },\n","        'vehicle_count': traci.vehicle.getIDCount(),\n","        'average_speed': traci.vehicle.getIDCount() > 0 and sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / traci.vehicle.getIDCount() or 0,\n","        'queue_lengths': {\n","            edge: traci.edge.getLastStepHaltingNumber(edge) for edge in\n","            ['1to7', '3to7', '5to8', 'e1to7', '7toe1', '2to8', '8to2', '8to7', '7to1', '7to4', '4to7', '7to3', '8to6', '6to8', '8to5']\n","        }\n","    }\n","\n","def take_action(action):\n","    if action == 0:\n","        traci.trafficlight.setPhase('7', (traci.trafficlight.getPhase('7') + 1) % 4)\n","    elif action == 1:\n","        traci.trafficlight.setPhase('8', (traci.trafficlight.getPhase('8') + 1) % 4)\n","    elif action == 2:\n","        traci.trafficlight.setPhase('7', (traci.trafficlight.getPhase('7') + 1) % 4)\n","        traci.trafficlight.setPhase('8', (traci.trafficlight.getPhase('8') + 1) % 4)\n","    # Action 3 does nothing (keeps current phases)\n","\n","def calculate_metrics():\n","    return {\n","        'average_waiting_time': sum(traci.vehicle.getAccumulatedWaitingTime(v) for v in traci.vehicle.getIDList()) / max(len(traci.vehicle.getIDList()), 1),\n","        'average_speed': sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / max(len(traci.vehicle.getIDList()), 1),\n","        'total_queue_length': sum(traci.edge.getLastStepHaltingNumber(edge) for edge in traci.edge.getIDList())\n","    }\n","\n","# # Run Q-learning\n","# Q, metrics_history = q_learning()\n","\n","# # Print final metrics\n","# final_metrics = metrics_history[-1]\n","# print(\"Final metrics after Q-learning:\")\n","# print(f\"Average waiting time: {final_metrics['average_waiting_time']:.2f} seconds\")\n","# print(f\"Average speed: {final_metrics['average_speed']:.2f} m/s\")\n","# print(f\"Total queue length: {final_metrics['total_queue_length']}\")\n","# print(f\"Total reward: {final_metrics['total_reward']:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"ZxSj6oTjDbZl"},"source":["# 5. Deep Q-Learning Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AqTr1qj8O5l"},"outputs":[],"source":["import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","import traci\n","\n","# Hyperparameters\n","EPSILON = 1.0  # Exploration rate\n","EPSILON_MIN = 0.01\n","EPSILON_DECAY = 0.995\n","DISCOUNT_FACTOR = 0.99\n","LEARNING_RATE = 0.001\n","NUM_EPISODES = 1000\n","BATCH_SIZE = 64\n","TARGET_UPDATE_FREQ = 10\n","MAX_MEMORY_SIZE = 10000\n","\n","# DQN Model (Neural Network for Q-function approximation)\n","class DQN(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, output_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Replay Buffer\n","class ReplayBuffer:\n","    def __init__(self):\n","        self.memory = deque(maxlen=MAX_MEMORY_SIZE)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def sample(self):\n","        return random.sample(self.memory, BATCH_SIZE)\n","\n","    def size(self):\n","        return len(self.memory)\n","\n","# Discretize State Function\n","def discretize_state(state):\n","    def categorize(value, ranges):\n","        for i, r in enumerate(ranges):\n","            if value <= r:\n","                return i\n","        return len(ranges)\n","\n","    discretized = [\n","        state['traffic_light_phases']['7'],\n","        state['traffic_light_phases']['8'],\n","        categorize(state['vehicle_count'], [10, 30, 40]),\n","        categorize(state['average_speed'], [0.5, 1, 2])\n","    ]\n","    discretized.extend([categorize(length, [0, 5, 8]) for length in state['queue_lengths'].values()])\n","    return discretized\n","\n","# Reward Calculation Function (using full state)\n","def calculate_reward(state):\n","    avg_speed = state['average_speed']\n","    total_queue = sum(state['queue_lengths'].values())\n","    return avg_speed - 0.1 * total_queue\n","\n","# Get State Function\n","def get_state():\n","    return {\n","        'traffic_light_phases': {\n","            '7': traci.trafficlight.getPhase('7'),\n","            '8': traci.trafficlight.getPhase('8')\n","        },\n","        'vehicle_count': traci.vehicle.getIDCount(),\n","        'average_speed': traci.vehicle.getIDCount() > 0 and sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / traci.vehicle.getIDCount() or 0,\n","        'queue_lengths': {\n","            edge: traci.edge.getLastStepHaltingNumber(edge) for edge in\n","            ['1to7', '3to7', '5to8', 'e1to7', '7toe1', '2to8', '8to2', '8to7', '7to1', '7to4', '4to7', '7to3', '8to6', '6to8', '8to5']\n","        }\n","    }\n","\n","\n","def take_action(action):\n","    if action == 0:\n","        traci.trafficlight.setPhase('7', (traci.trafficlight.getPhase('7') + 1) % 4)\n","    elif action == 1:\n","        traci.trafficlight.setPhase('8', (traci.trafficlight.getPhase('8') + 1) % 4)\n","    elif action == 2:\n","        traci.trafficlight.setPhase('7', (traci.trafficlight.getPhase('7') + 1) % 4)\n","        traci.trafficlight.setPhase('8', (traci.trafficlight.getPhase('8') + 1) % 4)\n","    # Action 3 does nothing (keeps current phases)\n","\n","def calculate_metrics():\n","    return {\n","        'average_waiting_time': sum(traci.vehicle.getAccumulatedWaitingTime(v) for v in traci.vehicle.getIDList()) / max(len(traci.vehicle.getIDList()), 1),\n","        'average_speed': sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / max(len(traci.vehicle.getIDList()), 1),\n","        'total_queue_length': sum(traci.edge.getLastStepHaltingNumber(edge) for edge in traci.edge.getIDList())\n","    }\n","\n","\n","# Deep Q-Learning Algorithm\n","def deep_q_learning(num_episodes=NUM_EPISODES, steps_per_episode=3600):\n","    input_dim = 5  # Dimension of the state space after discretization\n","    input_dim = 2 + 1 + 1 + 15  # 2 traffic lights, vehicle count, average speed, and queue lengths\n","    output_dim = 4  # Number of possible actions (0-3)\n","\n","    # Initialize Q-networks and optimizer\n","    policy_net = DQN(input_dim, output_dim).float()\n","    target_net = DQN(input_dim, output_dim).float()\n","    target_net.load_state_dict(policy_net.state_dict())  # Initialize target network\n","    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n","\n","\n","    # Experience replay buffer\n","    replay_buffer = ReplayBuffer()\n","\n","    # Metrics history\n","    metrics_history = []\n","\n","    global EPSILON\n","\n","    for episode in range(num_episodes):\n","        try:\n","            traci.start(['sumo', '-c', 'my_simulation.sumocfg'])\n","            state = get_state()  # Get the full state\n","            discretized_state = discretize_state(state)  # Get the discretized state\n","            total_reward = 0\n","            done = False\n","\n","            for step in range(steps_per_episode):\n","                # Select action using epsilon-greedy policy\n","                if random.random() < EPSILON:\n","                    action = random.choice([0, 1, 2, 3])\n","                else:\n","                    with torch.no_grad():\n","                        action = torch.argmax(policy_net(torch.tensor(discretized_state).float())).item()\n","\n","                take_action(action)  # Implement your action-taking logic here\n","                traci.simulationStep()\n","\n","                next_state = get_state()  # Get the full next state\n","                next_discretized_state = discretize_state(next_state)  # Get the discretized next state\n","                reward = calculate_reward(next_state)  # Use full next state for reward calculation\n","\n","                # Store transition in replay buffer\n","                replay_buffer.push(discretized_state, action, reward, next_discretized_state, done)\n","\n","                # Sample a batch from the replay buffer\n","                if replay_buffer.size() >= BATCH_SIZE:\n","                    batch = replay_buffer.sample()\n","                    states, actions, rewards, next_states, dones = zip(*batch)\n","\n","                    states = torch.tensor([list(s) for s in states]).float()\n","                    actions = torch.tensor(actions).long()\n","                    rewards = torch.tensor(rewards).float()\n","                    next_states = torch.tensor([list(s) for s in next_states]).float()\n","\n","                    # Compute Q-values for current states\n","                    q_values = policy_net(states)\n","                    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n","\n","                    # Compute Q-values for next states using target network\n","                    next_q_values = target_net(next_states)\n","                    next_q_value = next_q_values.max(1)[0]\n","\n","                    # Compute target Q-values\n","                    target_q_value = rewards + DISCOUNT_FACTOR * next_q_value\n","\n","                    # Compute loss\n","                    loss = nn.MSELoss()(q_value, target_q_value)\n","\n","                    # Optimize the model\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                discretized_state = next_discretized_state\n","                total_reward += reward\n","\n","            # Decay epsilon\n","            if EPSILON > EPSILON_MIN:\n","                EPSILON *= EPSILON_DECAY\n","\n","            # Update target network\n","            if episode % TARGET_UPDATE_FREQ == 0:\n","                target_net.load_state_dict(policy_net.state_dict())\n","\n","            metrics = calculate_metrics()  # Replace with the actual metric function\n","            metrics['episode'] = episode\n","            metrics['total_reward'] = total_reward\n","            metrics_history.append(metrics)\n","\n","            print(f\"Episode {episode} completed. Total reward: {total_reward}\")\n","\n","        except traci.exceptions.FatalTraCIError as e:\n","            print(f\"TraCI error in episode {episode}: {e}\")\n","        finally:\n","            traci.close()\n","\n","    return policy_net, metrics_history\n","\n","# # Run Deep Q-learning\n","# policy_net, metrics_history = deep_q_learning()\n","\n","# # Print final metrics\n","# final_metrics = metrics_history[-1]\n","# print(\"Final metrics after DQN: \")\n","# print(f\"Average waiting time: {final_metrics['average_waiting_time']:.2f} seconds\")\n","# print(f\"Average speed: {final_metrics['average_speed']:.2f} m/s\")\n","# print(f\"Total queue length: {final_metrics['total_queue_length']}\")\n","# print(f\"Total reward: {final_metrics['total_reward']:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yv42KnAdewLU"},"outputs":[],"source":["import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","import traci\n","\n","# Hyperparameters\n","EPSILON = 1.0  # Exploration rate\n","EPSILON_MIN = 0.01\n","EPSILON_DECAY = 0.995\n","DISCOUNT_FACTOR = 0.99\n","LEARNING_RATE = 0.005\n","NUM_EPISODES = 500\n","BATCH_SIZE = 64\n","TARGET_UPDATE_FREQ = 5\n","MAX_MEMORY_SIZE = 10000\n","CONVERGENCE_WINDOW = 100\n","CONVERGENCE_THRESHOLD = 0.01\n","reward_window = deque(maxlen=CONVERGENCE_WINDOW)\n","\n","# Check if CUDA is available and use GPU if possible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# DQN Model (Neural Network for Q-function approximation)\n","class DQN(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, output_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Replay Buffer\n","class ReplayBuffer:\n","    def __init__(self):\n","        self.memory = deque(maxlen=MAX_MEMORY_SIZE)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def sample(self):\n","        return random.sample(self.memory, BATCH_SIZE)\n","\n","    def size(self):\n","        return len(self.memory)\n","\n","# Discretize State Function\n","def discretize_state(state):\n","    def categorize(value, ranges):\n","        for i, r in enumerate(ranges):\n","            if value <= r:\n","                return i\n","        return len(ranges)\n","\n","    discretized = [\n","        state['traffic_light_phases']['7'],\n","        state['traffic_light_phases']['8'],\n","        categorize(state['vehicle_count'], [10, 30, 40]),\n","        categorize(state['average_speed'], [0.5, 1, 2])\n","    ]\n","    discretized.extend([categorize(length, [0, 5, 8]) for length in state['queue_lengths'].values()])\n","    return discretized\n","\n","# Reward Calculation Function (using full state)\n","def calculate_reward(state):\n","    avg_speed = state['average_speed']\n","    total_queue = sum(state['queue_lengths'].values())\n","    waiting_vehicles = sum(traci.edge.getLastStepHaltingNumber(edge) for edge in state['queue_lengths'])\n","    return avg_speed - 0.01 * total_queue - 0.2 * waiting_vehicles\n","\n","# Get State Function\n","def get_state():\n","    return {\n","        'traffic_light_phases': {\n","            '7': traci.trafficlight.getPhase('7'),\n","            '8': traci.trafficlight.getPhase('8')\n","        },\n","        'vehicle_count': traci.vehicle.getIDCount(),\n","        'average_speed': traci.vehicle.getIDCount() > 0 and sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / traci.vehicle.getIDCount() or 0,\n","        'queue_lengths': {\n","            edge: traci.edge.getLastStepHaltingNumber(edge) for edge in\n","            ['1to7', '3to7', '5to8', 'e1to7', '7toe1', '2to8', '8to2', '8to7', '7to1', '7to4', '4to7', '7to3', '8to6', '6to8', '8to5']\n","        }\n","    }\n","\n","def take_action(action):\n","    if action == 0:\n","        traci.trafficlight.setPhase('7', (traci.trafficlight.getPhase('7') + 1) % 4)\n","    elif action == 1:\n","        traci.trafficlight.setPhase('8', (traci.trafficlight.getPhase('8') + 1) % 4)\n","    elif action == 2:\n","        traci.trafficlight.setPhase('7', (traci.trafficlight.getPhase('7') + 1) % 4)\n","        traci.trafficlight.setPhase('8', (traci.trafficlight.getPhase('8') + 1) % 4)\n","    # Action 3 does nothing (keeps current phases)\n","\n","def calculate_metrics():\n","    return {\n","        'average_waiting_time': sum(traci.vehicle.getAccumulatedWaitingTime(v) for v in traci.vehicle.getIDList()) / max(len(traci.vehicle.getIDList()), 1),\n","        'average_speed': sum(traci.vehicle.getSpeed(v) for v in traci.vehicle.getIDList()) / max(len(traci.vehicle.getIDList()), 1),\n","        'total_queue_length': sum(traci.edge.getLastStepHaltingNumber(edge) for edge in traci.edge.getIDList())\n","    }\n","\n","# Deep Q-Learning Algorithm\n","def deep_q_learning(num_episodes=NUM_EPISODES, steps_per_episode=3600):\n","    input_dim = 5  # Dimension of the state space after discretization\n","    input_dim = 2 + 1 + 1 + 15  # 2 traffic lights, vehicle count, average speed, and queue lengths\n","    output_dim = 4  # Number of possible actions (0-3)\n","    CONVERGENCE_WINDOW = 100\n","    CONVERGENCE_THRESHOLD = 0.01\n","    reward_window = deque(maxlen=CONVERGENCE_WINDOW)\n","    prev_avg_reward = float('-inf')\n","    reward_mean = 0\n","    reward_std = 0\n","    episode_count = 0\n","    losses  = []\n","\n","    # Initialize Q-networks and optimizer\n","    policy_net = DQN(input_dim, output_dim).to(device)  # Move model to device (GPU/CPU)\n","    target_net = DQN(input_dim, output_dim).to(device)  # Move model to device (GPU/CPU)\n","    target_net.load_state_dict(policy_net.state_dict())  # Initialize target network\n","    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n","\n","    # Experience replay buffer\n","    replay_buffer = ReplayBuffer()\n","\n","    # Metrics history\n","    metrics_history = []\n","\n","    global EPSILON\n","\n","    for episode in range(num_episodes):\n","        try:\n","            traci.start(['sumo', '-c', 'my_simulation.sumocfg'])\n","            state = get_state()  # Get the full state\n","            discretized_state = discretize_state(state)  # Get the discretized state\n","            total_reward = 0\n","            done = False\n","\n","            for step in range(steps_per_episode):\n","                # Select action using epsilon-greedy policy\n","                if random.random() < EPSILON:\n","                    action = random.choice([0, 1, 2, 3])\n","                else:\n","                    with torch.no_grad():\n","                        action = torch.argmax(policy_net(torch.tensor(discretized_state).float().to(device))).item()  # Move tensor to GPU\n","\n","                take_action(action)  # Implement your action-taking logic here\n","                traci.simulationStep()\n","\n","                next_state = get_state()  # Get the full next state\n","                next_discretized_state = discretize_state(next_state)  # Get the discretized next state\n","                reward = calculate_reward(next_state)  # Use full next state for reward calculation\n","                episode_count += 1\n","                delta = reward - reward_mean\n","                reward_mean += delta / episode_count\n","                reward_std += delta * (reward - reward_mean)\n","                normalized_reward = (reward - reward_mean) / (np.sqrt(reward_std / episode_count) + 1e-5)\n","\n","                # Store transition in replay buffer\n","                replay_buffer.push(discretized_state, action, normalized_reward, next_discretized_state, done)\n","\n","                # Sample a batch from the replay buffer\n","                if replay_buffer.size() >= BATCH_SIZE:\n","                    batch = replay_buffer.sample()\n","                    states, actions, rewards, next_states, dones = zip(*batch)\n","\n","                    states = torch.tensor([list(s) for s in states]).float().to(device)  # Move tensors to GPU\n","                    actions = torch.tensor(actions).long().to(device)  # Move tensors to GPU\n","                    rewards = torch.tensor(rewards).float().to(device)  # Move tensors to GPU\n","                    next_states = torch.tensor([list(s) for s in next_states]).float().to(device)  # Move tensors to GPU\n","\n","                    # Compute Q-values for current states\n","                    q_values = policy_net(states)\n","                    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n","\n","                    # Compute Q-values for next states using target network\n","                    next_q_values = target_net(next_states)\n","                    next_q_value = next_q_values.max(1)[0]\n","\n","                    # Compute target Q-values\n","                    target_q_value = rewards + DISCOUNT_FACTOR * next_q_value\n","\n","                    # Compute loss\n","                    loss = nn.MSELoss()(q_value, target_q_value)\n","                    losses.append(loss)\n","\n","                    # Optimize the model\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                discretized_state = next_discretized_state\n","                total_reward += reward\n","\n","            # Decay epsilon\n","            if EPSILON > EPSILON_MIN:\n","                EPSILON *= EPSILON_DECAY\n","\n","            # Update target network\n","            if episode % TARGET_UPDATE_FREQ == 0:\n","                target_net.load_state_dict(policy_net.state_dict())\n","\n","            metrics = calculate_metrics()  # Replace with the actual metric function\n","            metrics['episode'] = episode\n","            metrics['total_reward'] = total_reward\n","            metrics_history.append(metrics)\n","\n","            print(f\"Episode {episode} completed. Total reward: {total_reward}\")\n","            reward_window.append(total_reward)\n","            if len(reward_window) == CONVERGENCE_WINDOW:\n","                avg_reward = sum(reward_window) / CONVERGENCE_WINDOW\n","                if episode > CONVERGENCE_WINDOW and abs(avg_reward - prev_avg_reward) < CONVERGENCE_THRESHOLD:\n","                    print(f\"Converged at episode {episode}\")\n","                    break\n","                prev_avg_reward = avg_reward\n","\n","        except traci.exceptions.FatalTraCIError as e:\n","            print(f\"TraCI error in episode {episode}: {e}\")\n","        finally:\n","            traci.close()\n","\n","    return policy_net, metrics_history, losses\n","\n","\n","# Run Deep Q-learning\n","policy_net, metrics_history, losses = deep_q_learning()\n","\n","# Print final metrics\n","final_metrics = metrics_history[-1]\n","print(\"Final metrics after DQN: \")\n","print(f\"Average waiting time: {final_metrics['average_waiting_time']:.2f} seconds\")\n","print(f\"Average speed: {final_metrics['average_speed']:.2f} m/s\")\n","print(f\"Total queue length: {final_metrics['total_queue_length']}\")\n","print(f\"Total reward: {final_metrics['total_reward']:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvfkJR3TcQ2m"},"outputs":[],"source":["traci.close()"]},{"cell_type":"markdown","metadata":{"id":"l0xhHC5WEA-9"},"source":["# 6. Results and Observations:\n","\n","1. Simulation Results-\n","\n","Simulation ended at time: 3600.00\n","Reason: TraCI requested termination.\n","Performance:\n"," Duration: 57.12s\n"," TraCI-Duration: 55.90s\n"," Real time factor: 63.0252\n"," UPS: 2195.430672\n","Vehicles:\n"," Inserted: 663 (Loaded: 1491)\n"," Running: 39\n"," Waiting: 828\n","Teleports: 67 (Jam: 7, Yield: 56, Wrong Lane: 4)\n","\n","Simulation metrics: {'average_waiting_time': 4226.389444444444, 'average_travel_time': 2583.2052777777776, 'average_speed': 0.4487425447642984, 'total_distance_travelled': 4173098.795994839, 'average_fuel_consumption': 36.47448543241946, 'average_co2_emission': 84847.17495936058, 'throughput': 0}\n","\n","\n","2. Q-Learning-\n","\n","Episode 99 completed. Total reward: -5400.5915304935415\n","Simulation ended at time: 3600.00\n","Reason: TraCI requested termination.\n","Performance:\n"," Duration: 12.84s\n"," TraCI-Duration: 12.00s\n"," Real time factor: 280.461\n"," UPS: 9656.746650\n","Vehicles:\n"," Inserted: 782 (Loaded: 1491)\n"," Running: 27\n"," Waiting: 709\n","Teleports: 36 (Jam: 8, Yield: 24, Wrong Lane: 4)\n","\n","Final metrics after Q-learning:\n","Average waiting time: 79.22 seconds\n","Average speed: 0.58 m/s\n","Total queue length: 18\n","Total reward: -5400.59\n","\n","\n","3. Deep Q-Learning-\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1fuaFUMwTlyN5nDI9IzVQ-UhnP8AqMXPG","timestamp":1733270277794},{"file_id":"1oZ1YQqNim4NRlJ9xZ4zAzaBGsADUdYdk","timestamp":1733244702157},{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/dql-simulation-short-c52fb436-f76b-4f85-b6fe-633274446a72.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241202/auto/storage/goog4_request&X-Goog-Date=20241202T163505Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1dd820441ac5b2f3de808e0200c7f04fc9d00e85fa969b877094d3929305e89808e7a4cc6ed4f6e399c98a112226f22884f712a01d06fd432a62c2018cdded9b59827031179427bd8b9a94120d90bfb8dc15f85f35c78552fed2c09c97d15f689b0e59eb4b43d9959399c605921c3c14f32050f325601e33ff9e9faa1550b317055c1c542f5405be019c4418b53cefe04b761dce41da6764af284d61078d5e33ad4d52d8047d1a182fa84bada28651ae8378f1abcc91de9758c34002093719eadbb664839373f07317c4d17a3bce6185421538092ff0936a9a26640c1e2c4ac91a8a95ca0a1ef9f138f66532f8a274a1453181feb6e9ef61ed62d91692b5abf0","timestamp":1733171732503}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}